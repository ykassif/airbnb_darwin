{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cybersecurity-excellence-awards.com/wp-content/uploads/2017/06/366812.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Darwin Supervised Classification Model Building </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to getting started, there are a few things you want to do:\n",
    "1. Set the dataset path.\n",
    "2. Enter your username and password to ensure that you're able to log in successfully\n",
    "\n",
    "Once you're up and running, here are a few things to be mindful of:\n",
    "1. For every run, look up the job status (i.e. requested, failed, running, completed) and wait for job to complete before proceeding. \n",
    "2. If you're not satisfied with your model and think that Darwin can do better by exploring a larger search space, use the resume function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'amb_sdk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-50cd87ac203a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mamb_sdk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msdk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDarwinSdk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'amb_sdk'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Login to Darwin**<br>\n",
    "Enter your registered username and password below to login to Darwin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Login\n",
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('yasser@utexas.edu', 'En5dC4ZGwL')\n",
    "\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Path** <br>\n",
    "In the cell below, set the path to your dataset, the default is Darwin's example datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'listings/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Upload and Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read dataset and view a file snippet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the dataset path, the next step is to upload the dataset from your local device to the server. <br> In the cell below, you need to specify the dataset_name if you want to use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1078</td>\n",
       "      <td>*UT/Hyde Park Craftsman Apartment</td>\n",
       "      <td>4635658</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78705</td>\n",
       "      <td>30.30123</td>\n",
       "      <td>-97.73674</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>208</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2265</td>\n",
       "      <td>Zen-East in the Heart of Austin</td>\n",
       "      <td>2466</td>\n",
       "      <td>Paddy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78702</td>\n",
       "      <td>30.27750</td>\n",
       "      <td>-97.71398</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5245</td>\n",
       "      <td>Green, Colorful, Clean &amp;  Cozy home</td>\n",
       "      <td>2466</td>\n",
       "      <td>Paddy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78702</td>\n",
       "      <td>30.27577</td>\n",
       "      <td>-97.71379</td>\n",
       "      <td>Private room</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-03-14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5456</td>\n",
       "      <td>Walk to 6th, Rainey St and Convention Ctr</td>\n",
       "      <td>8028</td>\n",
       "      <td>Sylvia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78702</td>\n",
       "      <td>30.26112</td>\n",
       "      <td>-97.73448</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>472</td>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5769</td>\n",
       "      <td>NW Austin Room</td>\n",
       "      <td>8186</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78729</td>\n",
       "      <td>30.45596</td>\n",
       "      <td>-97.78370</td>\n",
       "      <td>Private room</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>2019-02-24</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       name  host_id  host_name  \\\n",
       "0  1078          *UT/Hyde Park Craftsman Apartment  4635658      Tracy   \n",
       "1  2265            Zen-East in the Heart of Austin     2466      Paddy   \n",
       "2  5245        Green, Colorful, Clean &  Cozy home     2466      Paddy   \n",
       "3  5456  Walk to 6th, Rainey St and Convention Ctr     8028     Sylvia   \n",
       "4  5769                             NW Austin Room     8186  Elizabeth   \n",
       "\n",
       "   neighbourhood_group  neighbourhood  latitude  longitude        room_type  \\\n",
       "0                  NaN          78705  30.30123  -97.73674  Entire home/apt   \n",
       "1                  NaN          78702  30.27750  -97.71398  Entire home/apt   \n",
       "2                  NaN          78702  30.27577  -97.71379     Private room   \n",
       "3                  NaN          78702  30.26112  -97.73448  Entire home/apt   \n",
       "4                  NaN          78729  30.45596  -97.78370     Private room   \n",
       "\n",
       "   price  minimum_nights  number_of_reviews last_review  reviews_per_month  \\\n",
       "0     85               1                208  2017-07-14               1.63   \n",
       "1    225               2                 23  2018-09-16               0.19   \n",
       "2    100              28                  9  2018-03-14               0.07   \n",
       "3     95               2                472  2019-02-22               3.88   \n",
       "4     40               1                240  2019-02-24               2.21   \n",
       "\n",
       "   calculated_host_listings_count  availability_365  \n",
       "0                               3                43  \n",
       "1                               3               125  \n",
       "2                               3                 3  \n",
       "3                               1               302  \n",
       "4                               1                72  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'austin_listings.csv'\n",
    "df = pd.read_csv(os.path.join(path, dataset_name))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload dataset to Darwin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400: BAD REQUEST - {\"message\": \"Dataset already exists\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset\n",
    "status, dataset = ds.upload_dataset(os.path.join(path, dataset_name))\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8e80a408463e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"price\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/amb_sdk-1.0-py2.7.egg/amb_sdk/sdk.pyc\u001b[0m in \u001b[0;36mclean_data\u001b[0;34m(self, dataset_name, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;31m# Clean a data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroutes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_auth_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'parse'"
     ]
    }
   ],
   "source": [
    "# clean dataset\n",
    "import urllib\n",
    "target = \"price\"\n",
    "status, job_id = ds.clean_data(dataset_name, target = target)\n",
    "\n",
    "\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)AttributeError: 'module' object has no attribute 'parse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a model that will learn the class labels in the target column.<br> In the default cancer dataset, the target column is \"Diagnosis\". <br> You will have to specify your own target name for your custom dataset. <br> You can also increase max_train_time for longer training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400: BAD REQUEST - {\"message\": \"Dataset has not been cleaned. Please clean dataset before training model.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = target + \"_model0\"\n",
    "status, job_id = ds.create_model(dataset_names = dataset_name, \\\n",
    "                                 model_name =  model, \\\n",
    "                                 max_train_time = '00:02')\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Training (Optional)\n",
    "Run the following cell for extra training, no need to specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train some more\n",
    "status, job_id = ds.resume_training_model(dataset_names = dataset_name,\n",
    "                                          model_name = model,\n",
    "                                          max_train_time = '00:05')\n",
    "                                          \n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model\n",
    "Analyze model provides feature importance ranked by the model. <br> It indicates a general view of which features pose a bigger impact on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve feature importance of built model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the 10 most important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "**Perform model prediction on the the training dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download predictions from Darwin's server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots comparing predictions with actual target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform model prediction on a test dataset that wasn't used in training.** <br>\n",
    "Upload test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = 'cancer_test.csv'\n",
    "status, dataset = ds.upload_dataset(os.path.join(path, test_data))\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean test dataset\n",
    "status, job_id = ds.clean_data(test_data, target = target, model_name = model)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(test_data, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots comparing predictions with actual target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create plots comparing predictions with actual target\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "df = pd.read_csv(os.path.join(path,test_data))\n",
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out which machine learning model did Darwin use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type['description']['best_genome'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
